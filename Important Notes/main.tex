\documentclass{article}
\hbadness=10000

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{xcolor} % Add this package for \textcolor
\usepackage{fancyhdr} % Add this package
\usepackage{ulem} % Add this package for better underlining
\usepackage{graphicx} % Add this package for including images
\usepackage{hyperref}
\usepackage{listingsutf8}  % instead of just listings
\usepackage{listingsutf8} % UTF-8 capable listings
\lstset{
    breaklines=true,               % Enable line breaking
    inputencoding=utf8,            % Set UTF-8 input encoding
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{green!50!black},
    stringstyle=\color{orange},
    backgroundcolor=\color{gray!10},
    frame=single
}


\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    filecolor=blue,
    urlcolor=blue,
    pdfborder={0 0 0}
}

\title{\LARGE\bfseries Comprehensive Data Science Documentation}
\author{
    \textbf{Prosenjit Mondol} \\
    \normalsize Data Scientist \\
    \normalsize \href{mailto:prosenjit1156@email.com}{prosenjit1156@email.com}
}
\date{\today}


\pagestyle{fancy} % Enable fancy header
\fancyhead[L]{My Personal Documentations} % Left side header with documentation name
\fancyhead[C]{} % Center header empty
\fancyhead[R]{Prosenjit} % Right side header empty
\renewcommand{\headrulewidth}{1pt} % Add a horizontal line under the header


\begin{document}

\maketitle

\tableofcontents % <-- Add this line for list view

\newpage % Start a new page after the table of contents

\section{Data Preprocessing}
\begin{itemize}
    \item \textbf{\textcolor{blue}{\dotuline{Sampling}}}\\
        Sampling techniques are used to select a representative subset of data from a large population to reduce the computational complexity and improve the efficiency of the analysis.
    \item \textbf{\textcolor{blue}{\dotuline{Transformation}}}\\
    Transformation techniques involve manipulating raw data to create a single input, such as scaling, normalization, or encoding categorical data.
    \item \textbf{\textcolor{blue}{\dotuline{Denoising}}}\\
    Denoising techniques remove unwanted noise from the data that can lead to inaccurate results.
    \item \textbf{\textcolor{blue}{\dotuline{Imputation}}}\\
    Imputation techniques are used to fill in missing values in the data using statistical methods.
    \item \textbf{\textcolor{blue}{\dotuline{Feature extraction}}}\\
    Feature extraction techniques help to identify and extract relevant features from the data that are significant in a particular context.
    \item \textbf{\textcolor{blue}{\dotuline{Normalization}}}\\
    Normalization techniques are used to organize data for more efficient access and processing.

\end{itemize}

\section{Handle Categorical Data}

Categorical data is a type of data that represents qualitative or nominal characteristics, such as gender, occupation, Categorical data cannot be measured or compared using mathematical operations like addition or subtraction.
\subsection{Different Encoding Methods for Categorical Data}
\begin{itemize}
    \item \textbf{\textcolor{blue}{\dotuline{One-Hot Encoding}}}\\
    One-Hot Encoding creates a new binary column for each category.
    \begin{lstlisting}[language=Python, caption={Logistic Regression Example}, label={lst:logreg}, backgroundcolor=\color{gray!10}, frame=single, keywordstyle=\color{blue}\bfseries, commentstyle=\color{green!50!black}, stringstyle=\color{orange}]
X = pd.get_dummies(X)
print(X)
    \end{lstlisting}
    \item \textbf{\textcolor{blue}{\dotuline{Label Encoding}}}\\
    Label Encoding assigns a numerical value to each category.
    \begin{center}
        \includegraphics[width=0.5\textwidth]{label encoding} % Replace 'example-image' with your image filename (without extension)
    \end{center}
    \item \textbf{\textcolor{blue}{\dotuline{Binary Encoding}}}\\
    Binary Encoding creates new columns representing each category.
\end{itemize}

\subsection{Looking at null or missing values}
\begin{itemize}
    \item \textbf{\textcolor{blue}{\dotuline{Mean Imputation}}}\\
    Mean imputation is a simple and widely used method for filling in missing values. 
    \item \textbf{\textcolor{blue}{\dotuline{Mode Imputation}}}\\
    Mode imputation is a method for filling in missing values that is similar to mean imputation, but instead of using the mean, it uses the mode of the available values in a column.
    \item \textbf{\textcolor{blue}{\dotuline{K-Nearest Neighbor (KNN) Imputation}}}\\
    KNN imputation is a method for filling in missing values that is based on the distance between data poionts.
    \begin{center}
    \begin{lstlisting}[language=Python, caption={Logistic Regression Example}, label={lst:logreg}, backgroundcolor=\color{gray!10}, frame=single, keywordstyle=\color{blue}\bfseries, commentstyle=\color{green!50!black}, stringstyle=\color{orange}]
    # Multiple Imputation by Chained Equations
    from sklearn.experimental import enable_iterative_imputer
    from sklearn.impute import IterativeImputer

    #mputed_data = df[numerical_columns].copy(deep=True) 
    mice_imputer = IterativeImputer()
    data[numerical_columns] = mice_imputer.fit_transform(data[numerical_columns])
    \end{lstlisting}
    \end{center}
\end{itemize}

\section{Checking imblanced in target variable}
\begin{itemize}
    \item \textbf{\textcolor{blue}{\dotuline{Handling imbalanced data using oversampling}}}\\
    oversampling is a method for handling imbalanced data by increasing the size of the minority class.
    \begin{center}
    \begin{lstlisting}[language=Python, caption={Logistic Regression Example}, label={lst:logreg}, backgroundcolor=\color{gray!10}, frame=single, keywordstyle=\color{blue}\bfseries, commentstyle=\color{green!50!black}, stringstyle=\color{orange}]
from sklearn.utils import resample

no = normalized_data[normalized_data.RainTomorrow == 0]
yes = normalized_data[normalized_data.RainTomorrow == 1]
yes_oversampled = resample(yes, replace=True, n_samples=len(no), random_state=123)
oversampled_data = pd.concat([no, yes_oversampled])

fig = plt.figure(figsize = (8,5))
sns.countplot(x='RainTomorrow', data = oversampled_data, palette = "Set1").set(title='RainTomorrow Indicator No(0) and Yes(1) after Oversampling (Balanced Dataset)')
    \end{lstlisting}
    \end{center}
    
    \item \textbf{\textcolor{blue}{\dotuline{How multicollinearity affects decision trees}}}\\
    Multicollinearity affects decision trees by reducing the importance and accuracy of the input features.
    \begin{center}
    \begin{lstlisting}[language=Python, caption={Logistic Regression Example}, label={lst:logreg}, backgroundcolor=\color{gray!10}, frame=single, keywordstyle=\color{blue}\bfseries, commentstyle=\color{green!50!black}, stringstyle=\color{orange}]
    #the heat map of the correlation
    plt.figure(figsize=(16,10))
    sns.heatmap(X.corr(), annot=True, cmap='RdYlGn')
    \end{lstlisting}
    \end{center}
\end{itemize}

\section{Outliner Detection}
\begin{itemize}
    \item \textbf{\textcolor{blue}{\dotuline{Boxplot Method}}}\\
    One of the simplest and most popular methods for detecting outliers is the box-plot.
    \begin{center}
    \begin{lstlisting}[language=Python, caption={Logistic Regression Example}, label={lst:logreg}, backgroundcolor=\color{gray!10}, frame=single, keywordstyle=\color{blue}\bfseries, commentstyle=\color{green!50!black}, stringstyle=\color{orange}]
        plt.figure(figsize=(50,25))
        sns.boxplot(data=scaled_data[numerical_features])
    \end{lstlisting}
    \end{center}
    \newpage

    \item \textbf{\textcolor{blue}{\dotuline{Z-Score Method}}}\\
    The Z-Score method is a simple and widely used method for detecting outliers.
    \begin{center}
        \begin{lstlisting}[language=Python, caption={Logistic Regression Example}, label={lst:logreg}, backgroundcolor=\color{gray!10}, frame=single, keywordstyle=\color{blue}\bfseries, commentstyle=\color{green!50!black}, stringstyle=\color{orange}]
from scipy import stats
import numpy as np

# Calculate Z-scores for each value in the numerical features
z_scores = np.abs(stats.zscore(scaled_data[numerical_features]))

# Identify outliers (e.g., Z-score > 3)
outliers = (z_scores > 3)

# Print rows with outliers
print(scaled_data[outliers.any(axis=1)])
\end{lstlisting}
\end{center}
 \item \textbf{\textcolor{blue}{\dotuline{Transformation}}}\\
 Transformation involves transforming the data to a different scale to reduce the impact of the outliers.   
 \begin{center}
    \begin{lstlisting}[language=Python, caption={Logistic Regression Example}, label={lst:logreg}, backgroundcolor=\color{gray!10}, frame=single, keywordstyle=\color{blue}\bfseries, commentstyle=\color{green!50!black}, stringstyle=\color{orange}]
        from sklearn.preprocessing import PowerTransformer
        # Apply Power Transformation to the numerical features  
        power_transformer = PowerTransformer()
        scaled_data[numerical_features] = power_transformer.fit_transform(scaled_data[numerical_features])    
    \end{lstlisting}    
    \end{center}
\end{itemize}
\newpage


\section{Letâ€™s see how it fared in prediction using Logistic Regression}
\begin{center}
\begin{lstlisting}[language=Python, caption={Logistic Regression Example}, label={lst:logreg}, backgroundcolor=\color{gray!10}, frame=single, keywordstyle=\color{blue}\bfseries, commentstyle=\color{green!50!black}, stringstyle=\color{orange}]
# Train a logistic regression model on the training set
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression

# Instantiate the model
logreg = LogisticRegression(solver='liblinear', random_state=0)

# Fit the model
logreg.fit(X_train, y_train)

# Predict on the test set
y_pred_test = logreg.predict(X_test)

print('Model accuracy score: {0:0.4f}'.format(accuracy_score(y_test, y_pred_test)))
\end{lstlisting}
\end{center}
\vspace{1cm} % adjust cm as needed


\section{Algorithms}
\subsection{Simple Linear Regression}
The output is shown in the best fit line.
\begin{align*}
    y &= mx + C \\
    h_0(x) &= \theta_0 + \theta_1 x \\
    h_0(x) &= \hat{y}  \quad \text{(predicted value)} \\
    error= y - \hat{y} \\
\end{align*}
Here, $\theta_0$ is the intercept, $\theta_1$ is the slope or cofficient.\\
if $x=0$, then $h_0(x) = \theta_0$ (intercept).\\

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{best_fit.png} % Ensure this file exists in your folder
    \caption{Simple Linear Regression: Best Fit Line, Intercept, Slope, and Error}
    \label{fig:linear_regression}
\end{figure}
\vspace{1cm} % adjust cm as needed

\subsection{Convergence Algorithm \textit{(Optimize the changes of $\theta_1$ values)}}
Repeat until convergence:
\begin{align*}  
    \theta_j &= \theta_j - \alpha \cdot \frac{\partial J(\theta_j)}{\partial \theta_j} 
\end{align*}

\section{Handle missing values}
why it not

\newpage

\section{ Convolution Neural Networks (CNN)}
Convolutional Neural Network (CNN) is an advanced version of artificial neural networks (ANNs), primarily designed to extract features from grid-like matrix datasets. This is particularly useful for visual datasets such as images or videos, where data patterns play a crucial role. CNNs are widely used in computer vision applications due to their effectiveness in processing visual data.
\begin{center}
\begin{lstlisting}[language=Python, caption={Applying CNN to an Image}, label={lst:logreg}, backgroundcolor=\color{gray!10}, frame=single, keywordstyle=\color{blue}\bfseries, commentstyle=\color{green!50!black}, stringstyle=\color{orange}]
# import the necessary libraries
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from itertools import product

# set the param 
plt.rc('figure', autolayout=True)
plt.rc('image', cmap='magma')

# define the kernel
kernel = tf.constant([[-1, -1, -1],
                    [-1,  8, -1],
                    [-1, -1, -1],
                   ])

# load the image
image = tf.io.read_file('Ganesh.png')
image = tf.io.decode_jpeg(image, channels=1)
image = tf.image.resize(image, size=[300, 300])

# plot the image
img = tf.squeeze(image).numpy()
plt.figure(figsize=(5, 5))
plt.imshow(img, cmap='gray')
plt.axis('off')
plt.title('Original Gray Scale image')
plt.show();


# Reformat
image = tf.image.convert_image_dtype(image, dtype=tf.float32)
image = tf.expand_dims(image, axis=0)
kernel = tf.reshape(kernel, [*kernel.shape, 1, 1])
kernel = tf.cast(kernel, dtype=tf.float32)

# convolution layer
conv_fn = tf.nn.conv2d

image_filter = conv_fn(
    input=image,
    filters=kernel,
    strides=1, # or (1, 1)
    padding='SAME',
)

plt.figure(figsize=(15, 5))

# Plot the convolved image
plt.subplot(1, 3, 1)

plt.imshow(
    tf.squeeze(image_filter)
)
plt.axis('off')
plt.title('Convolution')

# activation layer
relu_fn = tf.nn.relu
# Image detection
image_detect = relu_fn(image_filter)

plt.subplot(1, 3, 2)
plt.imshow(
    # Reformat for plotting
    tf.squeeze(image_detect)
)

plt.axis('off')
plt.title('Activation')

# Pooling layer
pool = tf.nn.pool
image_condense = pool(input=image_detect, 
                             window_shape=(2, 2),
                             pooling_type='MAX',
                             strides=(2, 2),
                             padding='SAME',
                            )

plt.subplot(1, 3, 3)
plt.imshow(tf.squeeze(image_condense))
plt.axis('off')
plt.title('Pooling')
plt.show()
\end{lstlisting}
\end{center}





































\vspace{1cm} % adjust cm as needed


\section{Models}

\subsection{Visual Geometry Group (VGG)}
\noindent
\begin{center}
    \fcolorbox{blue}{blue!10}{
        \parbox{0.95\linewidth}{
            \textbf{Definition:} \\
           VGG (Visual Geometry Group) is a deep convolutional neural network architecture known for its effectiveness in image recognition tasks. It's characterized by its simple yet deep structure, utilizing small 3x3 convolutional filters repeatedly. VGG models, particularly VGG16 and VGG19, are widely used and have achieved notable performance on datasets like ImageNet. 
        }
    }
\end{center}
\subsubsection{VGG16}
\noindent
\begin{center}  
    \fcolorbox{blue}{blue!10}{
        \parbox{0.95\linewidth}{
            \textbf{Definition:} \\
            VGG16 is a specific variant of the VGG architecture with 16 layers (13 convolutional layers and 3 fully connected layers). It is known for its depth and simplicity, making it effective for image classification tasks.
        }
    }
\end{center}

\subsection{Ensemble Model}
\noindent
\begin{center}
    \fcolorbox{blue}{blue!10}{
        \parbox{0.95\linewidth}{
            \textbf{Definition:} \\
            An ensemble model in machine learning combines the predictions of multiple individual models (base estimators) to produce a more accurate and robust prediction than any single model alone.
        }
    }
\end{center}

\subsection{SOTA Model}
\noindent
\begin{center}
    \fcolorbox{blue}{blue!10}{
        \parbox{0.95\linewidth}{
            \textbf{Definition:} \\
            In deep learning, SOTA model means State-of-the-Art model â€” basically, the best-performing architecture or method for a given task at a given time, according to benchmarks or competitions.
        }
    }
\end{center}

\subsection{Utsu Method}
\noindent
\begin{center}
    \fcolorbox{blue}{blue!10}{
        \parbox{0.95\linewidth}{
            \textbf{Definition:} \\
            Otsu's method is a technique used in computer vision and image processing for automatic image thresholding.
        }
    }
\end{center}

\newpage % new page for better organization
\section{Plotting}


\end{document}
